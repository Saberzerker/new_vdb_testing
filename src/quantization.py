# src/quantization.py

"""
INT8 Quantization for Vector Compression

Reduces memory usage by 4× (FP32 → INT8) with minimal accuracy loss (<2%).

How it works:
- Calibrate on sample vectors to learn min/max ranges
- Quantize FP32 vectors to UINT8 (0-255 range)
- Dequantize for search (FAISS needs FP32)

Trade-off:
- Memory: 1.5MB → 384KB (4× reduction)
- Accuracy: >98% maintained
- Speed: Slightly slower (quantize/dequantize overhead)

Author: Saberzerker
Date: 2025-11-30
"""

import numpy as np
import logging
from typing import Tuple

logger = logging.getLogger(__name__)


class INT8Quantizer:
    """
    Quantize FP32 vectors to INT8 for 4× memory reduction.
    
    Uses min-max normalization per dimension:
    - Learn min/max from calibration set
    - Scale each dimension to [0, 255]
    - Store as uint8
    
    Accuracy loss: <2% on most datasets
    """

    def __init__(self):
        self.min_vals = None  # Per-dimension minimums
        self.max_vals = None  # Per-dimension maximums
        self.calibrated = False

    def calibrate(self, sample_vectors: np.ndarray):
        """
        Learn quantization parameters from sample vectors.
        
        Args:
            sample_vectors: (N, D) array of FP32 vectors
                           Should be representative of data distribution
                           Typically use 500-1000 samples
        """
        if sample_vectors.shape[0] == 0:
            logger.warning("[QUANT] Empty calibration set, skipping")
            return

        # Learn per-dimension min/max
        self.min_vals = np.min(sample_vectors, axis=0)
        self.max_vals = np.max(sample_vectors, axis=0)
        
        # Add small epsilon to avoid division by zero
        range_vals = self.max_vals - self.min_vals
        range_vals[range_vals < 1e-8] = 1.0
        self.max_vals = self.min_vals + range_vals
        
        self.calibrated = True
        
        logger.info(f"[QUANT] Calibrated on {len(sample_vectors)} vectors")
        logger.debug(f"[QUANT] Min range: [{self.min_vals.min():.3f}, {self.min_vals.max():.3f}]")
        logger.debug(f"[QUANT] Max range: [{self.max_vals.min():.3f}, {self.max_vals.max():.3f}]")

    def quantize(self, vectors: np.ndarray) -> np.ndarray:
        """
        Quantize FP32 → UINT8.
        
        Args:
            vectors: (N, D) FP32 array
        
        Returns:
            (N, D) UINT8 array (4× smaller!)
        """
        if not self.calibrated:
            raise ValueError("Must calibrate quantizer before use")

        # Normalize to [0, 1] using learned min/max
        normalized = (vectors - self.min_vals) / (self.max_vals - self.min_vals + 1e-8)
        
        # Clip to [0, 1] range (handle outliers)
        normalized = np.clip(normalized, 0.0, 1.0)
        
        # Scale to [0, 255] and convert to uint8
        quantized = (normalized * 255).astype(np.uint8)
        
        return quantized

    def dequantize(self, quantized: np.ndarray) -> np.ndarray:
        """
        Dequantize UINT8 → FP32 for search.
        
        Args:
            quantized: (N, D) UINT8 array
        
        Returns:
            (N, D) FP32 array (approximate reconstruction)
        """
        if not self.calibrated:
            raise ValueError("Must calibrate quantizer before use")

        # Convert to [0, 1]
        normalized = quantized.astype(np.float32) / 255.0
        
        # Denormalize using learned min/max
        vectors = normalized * (self.max_vals - self.min_vals) + self.min_vals
        
        return vectors

    def get_compression_ratio(self) -> float:
        """Get compression ratio (should be ~4.0)."""
        return 4.0  # FP32 (4 bytes) → UINT8 (1 byte)

    def get_memory_savings(self, num_vectors: int, dimension: int) -> dict:
        """
        Calculate memory savings.
        
        Args:
            num_vectors: Number of vectors
            dimension: Vector dimension
        
        Returns:
            Dict with memory statistics
        """
        fp32_bytes = num_vectors * dimension * 4  # 4 bytes per float32
        uint8_bytes = num_vectors * dimension * 1  # 1 byte per uint8
        
        savings_bytes = fp32_bytes - uint8_bytes
        savings_mb = savings_bytes / (1024**2)
        
        return {
            'fp32_mb': fp32_bytes / (1024**2),
            'uint8_mb': uint8_bytes / (1024**2),
            'savings_mb': savings_mb,
            'savings_percent': (savings_bytes / fp32_bytes * 100),
            'compression_ratio': fp32_bytes / uint8_bytes
        }

    def estimate_accuracy_loss(self, test_vectors: np.ndarray) -> dict:
        """
        Estimate accuracy loss on test vectors.
        
        Args:
            test_vectors: (N, D) FP32 test set
        
        Returns:
            Dict with accuracy metrics
        """
        if not self.calibrated:
            return {'error': 'Not calibrated'}

        # Quantize and dequantize
        quantized = self.quantize(test_vectors)
        reconstructed = self.dequantize(quantized)
        
        # Calculate reconstruction error
        mse = np.mean((test_vectors - reconstructed) ** 2)
        rmse = np.sqrt(mse)
        
        # Calculate relative error
        norm = np.linalg.norm(test_vectors, axis=1)
        reconstruction_norm = np.linalg.norm(reconstructed, axis=1)
        relative_error = np.mean(np.abs(norm - reconstruction_norm) / (norm + 1e-8))
        
        # Calculate cosine similarity (should be >0.98)
        dot_product = np.sum(test_vectors * reconstructed, axis=1)
        norm_product = norm * reconstruction_norm
        cosine_sim = np.mean(dot_product / (norm_product + 1e-8))
        
        return {
            'mse': mse,
            'rmse': rmse,
            'relative_error': relative_error,
            'cosine_similarity': cosine_sim,
            'accuracy_retained': cosine_sim * 100,  # Should be >98%
            'accuracy_loss': (1 - cosine_sim) * 100  # Should be <2%
        }
